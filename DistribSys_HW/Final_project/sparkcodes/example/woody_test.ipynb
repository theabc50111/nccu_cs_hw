{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13eb9b23-6198-4c72-95c8-22f8ef765994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SparkSession\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca0e132-63af-4f3a-a06a-54d35343c5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'app-20220529133038-0048'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/29 13:35:03 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "22/05/29 13:35:03 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:919)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:154)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:262)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:169)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/29 13:35:03 ERROR TaskSchedulerImpl: Lost executor 2 on 172.17.0.2: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/29 13:35:03 ERROR TaskSchedulerImpl: Lost executor 0 on 172.17.0.2: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/29 13:35:03 ERROR TaskSchedulerImpl: Lost executor 3 on 172.17.0.2: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/29 13:35:03 ERROR TaskSchedulerImpl: Lost executor 4 on 172.17.0.2: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/29 13:35:03 ERROR TaskSchedulerImpl: Lost executor 1 on 172.17.0.2: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d63b61d-533a-46fb-99d5-81a2b01cf6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/29 13:16:34 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              label|            features|\n",
      "+-------------------+--------------------+\n",
      "| -9.490009878824548|(10,[0,1,2,3,4,5,...|\n",
      "| 0.2577820163584905|(10,[0,1,2,3,4,5,...|\n",
      "| -4.438869807456516|(10,[0,1,2,3,4,5,...|\n",
      "|-19.782762789614537|(10,[0,1,2,3,4,5,...|\n",
      "| -7.966593841555266|(10,[0,1,2,3,4,5,...|\n",
      "| -7.896274316726144|(10,[0,1,2,3,4,5,...|\n",
      "| -8.464803554195287|(10,[0,1,2,3,4,5,...|\n",
      "| 2.1214592666251364|(10,[0,1,2,3,4,5,...|\n",
      "| 1.0720117616524107|(10,[0,1,2,3,4,5,...|\n",
      "|-13.772441561702871|(10,[0,1,2,3,4,5,...|\n",
      "| -5.082010756207233|(10,[0,1,2,3,4,5,...|\n",
      "|  7.887786536531237|(10,[0,1,2,3,4,5,...|\n",
      "| 14.323146365332388|(10,[0,1,2,3,4,5,...|\n",
      "|-20.057482615789212|(10,[0,1,2,3,4,5,...|\n",
      "|-0.8995693247765151|(10,[0,1,2,3,4,5,...|\n",
      "| -19.16829262296376|(10,[0,1,2,3,4,5,...|\n",
      "|  5.601801561245534|(10,[0,1,2,3,4,5,...|\n",
      "|-3.2256352187273354|(10,[0,1,2,3,4,5,...|\n",
      "| 1.5299675726687754|(10,[0,1,2,3,4,5,...|\n",
      "| -0.250102447941961|(10,[0,1,2,3,4,5,...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\").load(\"data/sample_linear_regression_data.txt\")\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3681184c-c713-410b-938a-c74aff8dab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.3229251667740594,-0.3438548034562219,1.915601702345841,0.05288058680386255,0.765962720459771,0.0,-0.15105392669186676,-0.21587930360904645,0.2202536918881343]\n",
      "Intercept: 0.15989368442397356\n",
      "numIterations: 6\n",
      "objectiveHistory: [0.49999999999999994, 0.4967620357443381, 0.49363616643404634, 0.4936351537897608, 0.4936351214177871, 0.49363512062528014, 0.4936351206216114]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  -9.889232683103197|\n",
      "|  0.5533794340053553|\n",
      "|  -5.204019455758822|\n",
      "| -20.566686715507508|\n",
      "|    -9.4497405180564|\n",
      "|  -6.909112502719487|\n",
      "|  -10.00431602969873|\n",
      "|  2.0623978070504845|\n",
      "|  3.1117508432954772|\n",
      "|  -15.89360822941938|\n",
      "|  -5.036284254673026|\n",
      "|  6.4832158769943335|\n",
      "|  12.429497299109002|\n",
      "|  -20.32003219007654|\n",
      "|    -2.0049838218725|\n",
      "| -17.867901734183793|\n",
      "|   7.646455887420495|\n",
      "| -2.2653482182417406|\n",
      "|-0.10308920436195645|\n",
      "|  -1.380034070385301|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 10.189077\n",
      "r2: 0.022861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aab1ca8-0c0c-4a55-bacf-3bfe504a52d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'app-20220529131632-0041'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86d02ddb-f186-46dd-b98a-2794428457bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a59261c7-21ba-4f7b-ada9-159f75d37468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.warehouse.dir',\n",
       "  'file:/root/DisSys_proj/sparkcodes/example/spark-warehouse'),\n",
       " ('spark.app.id', 'app-20220529131632-0041'),\n",
       " ('spark.executor.memory', '3g'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.driver.memory', '22g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.master', 'spark://eb6b917c950b:7077'),\n",
       " ('spark.driver.cores', '12'),\n",
       " ('spark.driver.host', 'eb6b917c950b'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '38103'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.startTime', '1653801391824')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd989d7e-c070-488c-b2b0-ecb595d00a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.setCommandRejectsSparkCoreConfs\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "046735d3-6f37-4665-8833-68b9af08230b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.driver.cores', '3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7385d5a5-ec0f-461c-a275-74a0f3174d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.warehouse.dir',\n",
       "  'file:/root/DisSys_proj/sparkcodes/example/spark-warehouse'),\n",
       " ('spark.app.id', 'app-20220529131632-0041'),\n",
       " ('spark.executor.memory', '3g'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.driver.memory', '22g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.master', 'spark://eb6b917c950b:7077'),\n",
       " ('spark.driver.cores', '12'),\n",
       " ('spark.driver.host', 'eb6b917c950b'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '38103'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.startTime', '1653801391824')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf0a6ac2-6a58-488e-94bd-f26d31d70fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "\n",
    "\n",
    "sc = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"test\")\\\n",
    "        .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a74cd906-fbf3-401c-a4f7-1138e26e48d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_conf = SparkConf().set('spark.driver.cores', '3').setAppName('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fd36c5c-fc31-44de-852a-5c1c3f5cadce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.memory', '3g'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.app.name', 'test'),\n",
       " ('spark.driver.memory', '22g'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.master', 'spark://eb6b917c950b:7077'),\n",
       " ('spark.driver.cores', '3')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_conf.getAll() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53b41391-88bb-4cc8-b024-ebe5608fc0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.warehouse.dir',\n",
       "  'file:/root/DisSys_proj/sparkcodes/example/spark-warehouse'),\n",
       " ('spark.app.id', 'app-20220529131632-0041'),\n",
       " ('spark.executor.memory', '3g'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.driver.memory', '22g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.master', 'spark://eb6b917c950b:7077'),\n",
       " ('spark.driver.cores', '12'),\n",
       " ('spark.driver.host', 'eb6b917c950b'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '38103'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.startTime', '1653801391824')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll() \n",
    "#sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4162d22b-200f-4a7b-8999-d529d43dfc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'app-20220529131643-0042'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.startTime', '1653801403740'),\n",
       " ('spark.driver.memory', '1g'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.master', 'spark://eb6b917c950b:7077'),\n",
       " ('spark.driver.cores', '3'),\n",
       " ('spark.driver.host', 'eb6b917c950b'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.executor.memory', '1g'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.name', 'test2'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '38813'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = pyspark.SparkConf().setAll([('spark.executor.memory', '1g'), \n",
    "                                     ('spark.driver.cores', '3'),  \n",
    "                                     ('spark.driver.memory','1g'), \n",
    "                                     ('spark.app.id','123'), ('spark.app.name','test2')])\n",
    "spark.sparkContext.stop()\n",
    "sc = pyspark.SparkContext(conf=config)\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9366f4df-c410-48db-8380-59d65dd3e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fc1f665-3020-4485-89dd-30d7e2432a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'app-20220529131643-0042'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.startTime', '1653801403740'),\n",
       " ('spark.driver.memory', '1g'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.master', 'spark://eb6b917c950b:7077'),\n",
       " ('spark.driver.cores', '3'),\n",
       " ('spark.driver.host', 'eb6b917c950b'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.executor.memory', '1g'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.name', 'test2'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '38813'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f8822-44ab-477b-87c0-1c3dac9dace3",
   "metadata": {},
   "source": [
    "# Boston House Price Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f63e25d-163d-4669-9e77-4e2b376d3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRIM: per capita crime rate by town.\n",
    "#ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "#INDUS: proportion of nonretail business acres per town.\n",
    "#CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "#NOX: nitric oxides concentration (parts per 10 million).\n",
    "#RM: average number of rooms per dwelling.\n",
    "#AGE: proportion of owner-occupied units built prior to 1940.\n",
    "#DIS: weighted distances to five Boston employment centers.\n",
    "#RAD: index of accessibility to radial highways.\n",
    "#TAX: full-value property-tax rate per $10,000.\n",
    "#PTRATIO: pupil-teacher ratio by town.\n",
    "#B: 1000(Bk – 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "#LSTAT: % lower status of the population.\n",
    "#MEDV: Median value of owner-occupied homes in $1000s.\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, DoubleType\n",
    "\n",
    "\n",
    "schema = StructType() \\\n",
    "      .add(\"CRIM\",DoubleType(),True) \\\n",
    "      .add(\"ZN\",DoubleType(),True) \\\n",
    "      .add(\"INDUS\",DoubleType(),True) \\\n",
    "      .add(\"CHAS\",DoubleType(),True) \\\n",
    "      .add(\"NOX\",DoubleType(),True) \\\n",
    "      .add(\"RM\",DoubleType(),True) \\\n",
    "      .add(\"AGE\",DoubleType(),True) \\\n",
    "      .add(\"DIS\",DoubleType(),True) \\\n",
    "      .add(\"RAD\",DoubleType(),True) \\\n",
    "      .add(\"TAX\",DoubleType(),True)\\\n",
    "      .add(\"PTRATIO\",DoubleType(),True)\\\n",
    "      .add(\"B\",DoubleType(),True)\\\n",
    "      .add(\"LSTAT\",DoubleType(),True)\\\n",
    "      .add(\"MEDV\",DoubleType(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c53e5b67-93a4-4c8b-95ae-7a9615d6a71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM|  AGE|   DIS|RAD|  TAX|PTRATIO|     B|LSTAT|MEDV|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+\n",
      "|0.00632|18.0| 2.31| 0.0|0.538|6.575| 65.2|  4.09|1.0|296.0|   15.3| 396.9| 4.98|24.0|\n",
      "|0.02731| 0.0| 7.07| 0.0|0.469|6.421| 78.9|4.9671|2.0|242.0|   17.8| 396.9| 9.14|21.6|\n",
      "|0.02729| 0.0| 7.07| 0.0|0.469|7.185| 61.1|4.9671|2.0|242.0|   17.8|392.83| 4.03|34.7|\n",
      "|0.03237| 0.0| 2.18| 0.0|0.458|6.998| 45.8|6.0622|3.0|222.0|   18.7|394.63| 2.94|33.4|\n",
      "|0.06905| 0.0| 2.18| 0.0|0.458|7.147| 54.2|6.0622|3.0|222.0|   18.7| 396.9| 5.33|36.2|\n",
      "|0.02985| 0.0| 2.18| 0.0|0.458| 6.43| 58.7|6.0622|3.0|222.0|   18.7|394.12| 5.21|28.7|\n",
      "|0.08829|12.5| 7.87| 0.0|0.524|6.012| 66.6|5.5605|5.0|311.0|   15.2| 395.6|12.43|22.9|\n",
      "|0.14455|12.5| 7.87| 0.0|0.524|6.172| 96.1|5.9505|5.0|311.0|   15.2| 396.9|19.15|27.1|\n",
      "|0.21124|12.5| 7.87| 0.0|0.524|5.631|100.0|6.0821|5.0|311.0|   15.2|386.63|29.93|16.5|\n",
      "|0.17004|12.5| 7.87| 0.0|0.524|6.004| 85.9|6.5921|5.0|311.0|   15.2|386.71| 17.1|18.9|\n",
      "|0.22489|12.5| 7.87| 0.0|0.524|6.377| 94.3|6.3467|5.0|311.0|   15.2|392.52|20.45|15.0|\n",
      "|0.11747|12.5| 7.87| 0.0|0.524|6.009| 82.9|6.2267|5.0|311.0|   15.2| 396.9|13.27|18.9|\n",
      "|0.09378|12.5| 7.87| 0.0|0.524|5.889| 39.0|5.4509|5.0|311.0|   15.2| 390.5|15.71|21.7|\n",
      "|0.62976| 0.0| 8.14| 0.0|0.538|5.949| 61.8|4.7075|4.0|307.0|   21.0| 396.9| 8.26|20.4|\n",
      "|0.63796| 0.0| 8.14| 0.0|0.538|6.096| 84.5|4.4619|4.0|307.0|   21.0|380.02|10.26|18.2|\n",
      "|0.62739| 0.0| 8.14| 0.0|0.538|5.834| 56.5|4.4986|4.0|307.0|   21.0|395.62| 8.47|19.9|\n",
      "|1.05393| 0.0| 8.14| 0.0|0.538|5.935| 29.3|4.4986|4.0|307.0|   21.0|386.85| 6.58|23.1|\n",
      "| 0.7842| 0.0| 8.14| 0.0|0.538| 5.99| 81.7|4.2579|4.0|307.0|   21.0|386.75|14.67|17.5|\n",
      "|0.80271| 0.0| 8.14| 0.0|0.538|5.456| 36.6|3.7965|4.0|307.0|   21.0|288.99|11.69|20.2|\n",
      "| 0.7258| 0.0| 8.14| 0.0|0.538|5.727| 69.5|3.7965|4.0|307.0|   21.0|390.95|11.28|18.2|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "boston_housing= spark.read.csv('data/boston_housing.csv', sep=',',header=False, schema = schema)\n",
    "boston_housing = boston_housing.dropna()\n",
    "boston_housing.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af28080-868a-46a6-9804-f0fa4ef7520b",
   "metadata": {},
   "source": [
    "### Summarize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb39086-ef48-4444-8176-d56bf4d7a3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|              CRIM|                ZN|               AGE|              MEDV|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|               452|               452|               452|               452|\n",
      "|   mean|1.4208250442477868|12.721238938053098| 65.55796460176992|23.750442477876135|\n",
      "| stddev|2.4958943920051566| 24.32603179418856|28.127025034855407| 8.808601660786652|\n",
      "|    min|           0.00632|               0.0|               2.9|               6.3|\n",
      "|    25%|           0.06911|               0.0|              40.5|              18.5|\n",
      "|    50%|           0.19073|               0.0|              71.7|              21.9|\n",
      "|    75%|           1.20742|              20.0|              91.6|              26.6|\n",
      "|    max|           9.96654|             100.0|             100.0|              50.0|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boston_housing.select('CRIM', 'ZN','AGE', 'MEDV').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5716d2-cf5d-4275-9880-f43a2dfb654d",
   "metadata": {},
   "source": [
    "### Create an assembler object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3384f5f-7dc2-4264-9fd7-c5f1a3463aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/29 13:27:38 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+----+\n",
      "|features                                                                 |MEDV|\n",
      "+-------------------------------------------------------------------------+----+\n",
      "|[0.00632,18.0,2.31,0.0,0.538,6.575,65.2,4.09,1.0,296.0,15.3,396.9,4.98]  |24.0|\n",
      "|[0.02731,0.0,7.07,0.0,0.469,6.421,78.9,4.9671,2.0,242.0,17.8,396.9,9.14] |21.6|\n",
      "|[0.02729,0.0,7.07,0.0,0.469,7.185,61.1,4.9671,2.0,242.0,17.8,392.83,4.03]|34.7|\n",
      "|[0.03237,0.0,2.18,0.0,0.458,6.998,45.8,6.0622,3.0,222.0,18.7,394.63,2.94]|33.4|\n",
      "|[0.06905,0.0,2.18,0.0,0.458,7.147,54.2,6.0622,3.0,222.0,18.7,396.9,5.33] |36.2|\n",
      "+-------------------------------------------------------------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "boston_assembled = assembler.transform(boston_housing)\n",
    "\n",
    "# Check the resulting column\n",
    "boston_assembled.select('features', 'MEDV').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d12ca-0982-4dfb-a6c3-953104ab7a13",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c9053a-0b0b-470a-8c37-beea34c20251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7853982300884956\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets in a 80:20 ratio\n",
    "boston_train, boston_test = boston_assembled.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = boston_train.count() / boston_housing.count()\n",
    "print(training_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4345fd8-8022-4f41-8d8a-354c89d7d9bd",
   "metadata": {},
   "source": [
    "### Train linear regression model and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b92582d-23fc-4fc8-8100-b421ff3735bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|MEDV|prediction        |\n",
      "+----+------------------+\n",
      "|24.0|30.254391159626515|\n",
      "|50.0|39.95712392178673 |\n",
      "|44.0|36.31241607566815 |\n",
      "|31.1|31.58506633539052 |\n",
      "|16.5|24.1436147841959  |\n",
      "+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.000236529534943"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='MEDV',regParam=0.3).fit(boston_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(boston_test)\n",
    "predictions.select('MEDV', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol='MEDV').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8943e7c-bcb9-4f90-a946-bac7023f8c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
