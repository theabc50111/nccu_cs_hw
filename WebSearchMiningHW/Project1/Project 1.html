
<!-- saved from url=(0076)https://wm5.nccu.edu.tw/base/10001/course/10025914/content/proj01/index.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    

    <title>Project 1</title>
    <style type="text/css">
        body
        {
            margin-top: 15px;
            margin-right: 15px;
            margin-bottom: 15px;
            margin-left: 15px;
            color: #000066;
            background-color: #FFFFFF;
            font-family: "verdana"; serif;
        }
    </style>
</head>

<body>
<h1 align="center">WSM Project 1: Ranking by Vector Space Models</h1>
<hr color="blue" size="10">

<ol>
<li>
[70 points] <strong>Vector Space Model with Different Weighting Schemes &amp; Similarity Metrics</strong>

<p>
The example codes given in Week 4 demonstrate how an IR system works via Vector Space Model. Below are some steps in the codes:
</p>

<ol>
    <li>Stemming &amp; Removing Stop Words (<a href="https://wm5.nccu.edu.tw/base/10001/course/10025914/content/proj01/EnglishStopwords.txt" target="_blank">English Stop Words</a>; <a href="https://wm5.nccu.edu.tw/base/10001/course/10025914/content/proj01/ChineseStopwords.txt" target="_blank">Chinese Stop Words</a>) &amp; Indexing</li>
    <li>Transfer Queries into a Vector</li>
    <li>Transfer Documents into Vectors</li>
    <li>Calculate the Similarity between the Query Vector and the Document Vectors</li>
    <li>Rank the Documents according to the Similarity scores</li>
</ol>

<p>
Now you are asked to develop a retrieval program that is able to retrieve the relevant news to the given query from a set of 7,034 <a href="https://wm5.nccu.edu.tw/base/10001/course/10025914/content/proj01/EnglishNews.zip">English News</a> collected from <i>reuters.com</i> according to different weighting schemes and similarity metrics. In the given dataset, each file is named by its News ID and contains the corresponding news title and content, as shown in below:
</p>
<center><img src="./Project 1_files/EnglishNews.png" alt="" height="150"></center>

<p>
There are the four combinations you're asked to implement. For each combination, please retrieve the top 10 results and scores. Here is an example result for the query "<b>Trump Biden Taiwan China</b>":
</p>

<ul>
    <li>[15/70 points] <strong> Term Frequency (TF) Weighting + Cosine Similarity</strong></li>
    <br>
    <img src="./Project 1_files/EnglishNews_TF_cosine.png" alt="" height="250"><br>
    <br>
    <li>[15/70 points] <strong> Term Frequency (TF) Weighting + Euclidean Distance </strong></li>
    <br>
    <img src="./Project 1_files/EnglishNews_TF_Euclidean.png" alt="" height="250"><br>
    <br>
    <li>[20/70 points] <strong> TF-IDF Weighting + Cosine Similarity </strong></li>
    <br>
    <img src="./Project 1_files/EnglishNews_TFIDF_cosine.png" alt="" height="250"><br>
    <br>
    <li>[20/70 points] <strong> TF-IDF Weighting + Euclidean Distance </strong></li>
    <br>
    <img src="./Project 1_files/EnglishNews_TFIDF_Euclidean.png" alt="" height="250"><br>
    <br>
</ul>
<p>&nbsp;</p>
</li>

<li>
[30 points] <strong> Relevance Feedback </strong>

<p>
Relevance Feedback is an IR technique for improving retrieved results. The simplest approach is Pseudo Feedback, the idea of which is to feed the results retrieved by the given query, and then to use the content of the fed results as supplement queries to re-score the documents.
</p>

<p>
In this work, you're asked to use the Nouns and the Verbs within the first document of the above <b>Method 3</b> (e.g. TF-IDF Weighting + Cosine Similarity ) for Pseudo Feedback. The new query term weighting scheme is <strong>[1 * original query + 0.5 * feedback query]</strong>. Please try to use the new query to re-rank the documents.
</p>

<p>
For instance, suppose the index vector is ["network", "computer", "share", "ask", "soccer", "song"], the query is "network", and the content of the feedback document is:
</p>
<dl>
<dd><strong>Jimmy shares songs via the computer network.</strong></dd>
</dl>

<p>
Then we will get a new query vector like this:
</p><dl>
<dd>1 * [1, 0, 0, 0, 0, 0] + 0.5 * [1, 1, 1, 0, 0, 1] = [1.5, 0.5, 0.5, 0, 0, 0.5]</dd>
</dl>
<p></p>

<p>
In this work, you may need to use the Python NLTK package. For more details, please refer to <a href="http://nltk.org/" target="_blank">this link</a>.
</p>
</li>

<li>
[30 points] <strong> Vector Space Model with Different Scheme &amp; Similarity Metrics in Chinese and English</strong>

<p>
In this part, you are asked to retrieve the relevant news to the query from a set of 1,500 <a href="https://wm5.nccu.edu.tw/base/10001/course/10025914/content/proj01/News.zip" target="_blank">News</a> (1000 Chinese news and 500 English news) collected from <i>chinatimes.com</i> and <i>setn.com</i> according to different weighting schemes (TF and TF-IDF) and <strong>cosine similarity metric</strong>.
</p>
<p>
Here is the example result of the query "烏克蘭 大選":
</p>
<img src="./Project 1_files/News.png" alt="" height="250">
<p>
Hint: You may use <u>Jieba</u> or <u>CKIP</u> to split the Chinese word segments.
</p>
</li></ol>
<!--
<p>
Appendix. <a href="http://nltk.org/"><strong>Python Natural Language Toolkit</strong></a><br><br>
NLTK is a Python package about Natural Language Processing. 
You can use the toolkit to find out the Nouns and the Verbs among a document. 
The official website offers a good example about how to tag the linguistic category of words. 
Please refer the links to see more details.
<br><br>
Tips. <br><br>
The example code and NLTK utilize the different stemming methods. 
It may leads to some troubles while maintaining the program. 
Hence you are encouraged to use only the NLTK tokenizer throughout the project. 
</p>
-->  <hr color="blue" size="10">
<h3>Submission Details</h3>
<ul>
    <li>Due: 23:59, Tuesday, 29 March 2022</li>
    <li>What to turn in: <br><br>
    Electrical submission: compress all the necessary fiels and data into a zip file, and submit it via the WM5 website.
    </li>
    <br>
    <li>Late policy:
    <p>
    In general, late homework may receive fewer points than incomplete homework. 
    The penalty for late homework is about 20 points per day. 
    Please DO comment and format your codes to avoid any penalty imposed by the grader.
    </p>
    </li>
</ul>
<div id="xunlei_com_thunder_helper_plugin_d462f475-c18e-46be-bd10-327458d045bd"></div>



</body></html>